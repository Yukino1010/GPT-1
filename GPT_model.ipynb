{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","\n","import os\n","import tensorflow as tf\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","EPOCHS =100\n","BATCH_SIZE = 32\n","LR = 2e-5\n","D_MODEL = 768\n","DFF = 2048\n","N_HEAD = 12\n","N_LAYER = 6\n","SAVE_EVERY_EPOCHS = 5\n","INFER_EVERY_EPOCHS = 20\n","INFER_MAX_LEN = 50\n","LOG_DIR = './logs/'\n","OUTPUT_PATH = r'./out_weight'\n","\n","if not os.path.exists(OUTPUT_PATH):\n","    os.mkdir(OUTPUT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["word2id, id2word = json.load(open('./dict_data.json', 'r'))\n","vocab_size = len(word2id)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","print(vocab_size)\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_data(datas):\n","    train_datas = []\n","    for data in datas:\n","        data = data.strip()\n","        train_data = [i if i != '\\t' else \"<sep>\" for i in data]+[\"<sep>\"]\n","        train_datas.append(train_data)\n","\n","    train_data_num = [[word2id[word] for word in line] for line in train_datas]\n","    return train_data_num\n","\n","class BuildDataset(Dataset):\n","    def __init__(self, datas):\n","        self.datas = datas\n","\n","    def __getitem__(self, idx):\n","        data = self.datas[idx]\n","        decoder_inp = data[:-1]\n","        decoder_out = data[1:]\n","        return (decoder_inp, decoder_out)\n","    \n","    def __len__(self):\n","        return len(self.datas)\n","    \n","    def padding_batch(self, batch):\n","        inp_len = list(map(lambda i:len(i[0]), batch))\n","        out_len = list(map(lambda i:len(i[1]), batch))\n","\n","        max_inp_len = max(inp_len)\n","        max_out_len = max(out_len)\n","\n","        for i, data in enumerate(batch):\n","            data[0].extend([word2id[\"<pad>\"]]*(max_inp_len - inp_len[i]))\n","            data[1].extend([word2id[\"<pad>\"]]*(max_out_len - out_len[i]))\n","        \n","        decoder_inp = torch.LongTensor([d[0] for d in batch])\n","        decoder_out = torch.LongTensor([d[1] for d in batch])\n","        return decoder_inp, decoder_out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_attn_pad_mask(seq_q, seq_k):\n","    len_q = seq_q.size(1)\n","    len_k = seq_k.size(1)\n","    pad_mask = seq_k.data.eq(0).unsqueeze(1)\n","    return pad_mask.expand(seq_q.size(0), len_q, len_k)\n","\n","def get_attn_subsequence_mask(seq):\n","    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n","    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n","    subsequence_mask = torch.from_numpy(subsequence_mask).to(device)\n","    return subsequence_mask\n","\n","\n","def ScaleDotProduct(q, k, v, attn_mask=None):\n","    d_k = k.size(-1)\n","    matmul_qk = torch.matmul(q, k.transpose(-1, -2)/np.sqrt(d_k))\n","    if attn_mask is not None:\n","        matmul_qk.masked_fill_(attn_mask, -1e9)\n","\n","    attn_weights = nn.Softmax(dim=-1)(matmul_qk)\n","    out = torch.matmul(attn_weights, v)\n","    return out, attn_weights\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.d_heads = d_model // n_heads\n","\n","        self.w_q = nn.Linear(self.d_model, self.d_heads * self.n_heads, bias=False)\n","        self.w_k = nn.Linear(self.d_model, self.d_heads * self.n_heads, bias=False)\n","        self.w_v = nn.Linear(self.d_model, self.d_heads * self.n_heads, bias=False)\n","        self.out_linear = nn.Linear(self.d_model, self.d_model, bias=False)\n","        self.layer_norm = nn.LayerNorm(self.d_model)\n","    \n","    def forward(self, q, k, v, attn_mask=None):\n","        residual, batch_size = q, q.size(0)\n","\n","        Q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_heads).transpose(1, 2)\n","        K = self.w_q(k).view(batch_size, -1, self.n_heads, self.d_heads).transpose(1, 2)\n","        V = self.w_q(v).view(batch_size, -1, self.n_heads, self.d_heads).transpose(1, 2)\n","        if attn_mask is not None:\n","            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n","        \n","        context, attn_map = ScaleDotProduct(Q, K, V, attn_mask)\n","        context = context.permute(0, 2, 1, 3)\n","        context = context.reshape(batch_size, -1, self.n_heads * self.d_heads)\n","\n","        out = self.out_linear(context)\n","        return self.layer_norm(out + residual), attn_map\n","\n","class FFN(nn.Module):\n","    def __init__(self, d_model, dff):\n","        super(FFN, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(d_model, dff, bias=False),\n","            nn.ReLU(),\n","            nn.Linear(dff, d_model, bias=False)\n","        )\n","        self.layer_norm = nn.LayerNorm(d_model)\n","    def forward(self, inp):\n","        residual = inp\n","        out = self.fc(inp)\n","        return self.layer_norm(residual + out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, dff, n_head):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, n_head)\n","        self.ffn = FFN(d_model, dff)\n","    \n","    def forward(self, x, attn_mask):\n","        dec_out, attn_map = self.self_attn(x, x, x, attn_mask)\n","        dec_out = self.ffn(dec_out)\n","        return dec_out, attn_map\n","\n","class Decoder(nn.Module):\n","    def __init__(self, d_model, dff, n_head, n_layer):\n","        super(Decoder, self).__init__()\n","\n","        self.emb = nn.Embedding(vocab_size, d_model)\n","        self.pos_emb = nn.Embedding(300, d_model)\n","        self.layers = nn.ModuleList([\n","            DecoderLayer(d_model, dff, n_head)\\\n","                  for _ in range(n_layer)\n","        ])\n","    \n","    def forward(self, inp):\n","        seq_len = inp.size(1)\n","        pos = torch.arange(seq_len).to(device)\n","        pos = pos.unsqueeze(0).expand_as(inp)\n","\n","        dec_inp = self.emb(inp) + self.pos_emb(pos)\n","\n","        attn_pad_mask = get_attn_pad_mask(inp, inp)\n","        attn_subsequence_mask = get_attn_subsequence_mask(inp)\n","        dec_total_mask = torch.gt((attn_pad_mask+attn_subsequence_mask), 0)\n","\n","        attn_maps = []\n","        for layer in self.layers:\n","            dec_inp, dec_attn_map = layer(dec_inp, dec_total_mask)\n","            attn_maps.append(dec_attn_map)\n","        \n","        return dec_inp, attn_maps\n","\n","class GPT(nn.Module):\n","    def __init__(self, d_model, dff, n_head, n_layer):\n","        super(GPT, self).__init__()\n","        self.decoder = Decoder(d_model, dff, n_head, n_layer)\n","        self.proj = nn.Linear(d_model, vocab_size)\n","    \n","    def forward(self, inp):\n","        dec_out, dec_attn = self.decoder(inp)\n","        dec_logit = self.proj(dec_out)\n","        return dec_logit.view(-1, vocab_size), dec_attn\n","    \n","    def greedy_decode(self, dec_inp, max_len):\n","        start_len = len(dec_inp[0])\n","        \n","        while True:\n","            if len(dec_inp[0]) - start_len > max_len:\n","                next_word = word2id['<sep>']\n","                dec_inp = torch.cat(\n","                    [dec_inp.detach(), torch.LongTensor([[next_word]]).to(device)], -1\n","                )\n","                break\n","            dec_out, _ = self.decoder(dec_inp)\n","            dec_proj = self.proj(dec_out)\n","            prob = dec_proj.squeeze(0).max(dim=-1)[1]\n","            next_word = prob.data[-1]\n","\n","            dec_inp = torch.cat(\n","                [dec_inp.detach(), torch.LongTensor([[next_word]]).to(device)], -1\n","            )\n","            \n","            if next_word == word2id['<sep>']:\n","                break\n","        return dec_inp\n","\n","    def answer(self, sentence, max_len):\n","        dec_inp = [word2id.get(word, 1) if word !='\\t' else \\\n","                   word2id['<sep>'] for word in sentence]\n","        dec_inp = torch.LongTensor(dec_inp).to(device).unsqueeze(0)\n","\n","        out = self.greedy_decode(dec_inp, max_len).squeeze()\n","        out = [id2word[int(i)] for i in out]\n","\n","        sep_index = []\n","        for i in range(len(out)):\n","            if out[i] == '<sep>':\n","                sep_index.append(i)\n","        print(sep_index)\n","        answer = out[sep_index[-2]+1:-1]\n","        answer = \"\".join(answer)\n","        return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open(\"./text_data.txt\", 'r', encoding='utf-8') as f:\n","    datas = f.readlines()\n","\n","dataset = BuildDataset(make_data(datas))\n","data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, \\\n","                         collate_fn=dataset.padding_batch, shuffle=True)\n","\n","model = GPT(D_MODEL, DFF, N_HEAD, N_LAYER).to(device)\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","summary_Writer = tf.summary.create_file_writer(LOG_DIR)\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","from tqdm import tqdm\n","\n","avg_losses = []\n","out_sentences = []\n","for epoch in range(1, EPOCHS+1):\n","    start = time.time()\n","    losses = []\n","\n","    for i, (inp_seq, tgt_seq) in enumerate(tqdm(data_loader)):\n","        \n","        optimizer.zero_grad()\n","        inp_seq, tgt_seq = inp_seq.to(device), tgt_seq.to(device)\n","        dec_out, attn_map = model(inp_seq)\n","        loss = criterion(dec_out, tgt_seq.view(-1))\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        \n","        if i > 3000:\n","            break\n","\n","    avg_loss =np.mean(losses)\n","    avg_losses.append(avg_loss)\n","    clear_output()\n","    \n","    with summary_Writer.as_default():\n","        tf.summary.scalar('loss', avg_loss, step=epoch)\n","    \n","    if epoch % SAVE_EVERY_EPOCHS == 0:\n","        end = time.time()\n","        print(\"curr loss: {:.3f}\".format(avg_loss))\n","        print(\"curr epoch: {}, time: {:.2f} min\".format(epoch, (end-start)/60))\n","        \n","        torch.save(model.state_dict(), \\\n","                os.path.join(OUTPUT_PATH, \\\n","                \"epoch_{}_loss_{:.3f}.dat\".format(epoch, avg_loss)))  \n","\n","    if epoch % INFER_EVERY_EPOCHS == 0:\n","        inp_temp = \"你好～～\" + \"\\t\"\n","        model_out = model.answer(inp_temp, INFER_MAX_LEN)\n","        out_sentences.append(model_out)\n","        print(\"chatbot:\", model_out)\n","              \n","    end = time.time()\n","    print(\"curr epoch: {}, time: {:.2f} min\".format(epoch, (end-start)/60))\n","\n","print('Finished Training')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(avg_losses)\n","plt.title(\"GPT_loss\")\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for sentence in out_sentences:\n","    print(\"chatbot:\", sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","PATH = \"Put your model weight here!!!\"\n","sentences = ''\n","model.load_state_dict(torch.load(PATH))\n","model.eval()\n","while True:\n","    print(\"input q to quit !!\")\n","    inp = input(\"pleas input your sentence:\")\n","    if inp == \"q\":\n","        break\n","    \n","    sentences = sentences + inp + '\\t'\n","    with torch.no_grad():\n","        model_out = model.answer(sentences, INFER_MAX_LEN)\n","    \n","    print(\"inp: {}\".format(sentences))\n","    print('model out:{}'.format(model_out))\n","    \n","    sentences += model_out\n","    sentences += '\\t'\n","    \n","'''"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
